{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09e5a098",
   "metadata": {},
   "source": [
    "# This is a full pipeline with keypoints prediction using VRNN in transfer mode for VoxCeleb dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4af7e9",
   "metadata": {},
   "source": [
    "# Import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa4a0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys  \n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from Training_Prediction.FOMM.Source_Model.logger import Logger, Visualizer\n",
    "import numpy as np\n",
    "import imageio\n",
    "from Training_Prediction.FOMM.Source_Model.sync_batchnorm import DataParallelWithCallback\n",
    "from Training_Prediction.FOMM.Source_Model.modules.RNN_prediction_module import PredictionModule\n",
    "from Training_Prediction.FOMM.Source_Model.augmentation import SelectRandomFrames, SelectFirstFrames_two, VideoToTensor\n",
    "from tqdm import trange\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from Training_Prediction.FOMM.Source_Model.frames_dataset import FramesDataset_transfer\n",
    "import tensorflow.compat.v1 as tf\n",
    "from Training_Prediction.PREDICTOR.Source_Model.VRNN import build_vrnn, get_config\n",
    "from Training_Prediction.PREDICTOR.Source_Model.VRNN_pytorch import VRNN, get_config, init_weights\n",
    "import pickle\n",
    "from Training_Prediction.PREDICTOR.Source_Model.VRNN_prediction import VRNN_predict\n",
    "from Training_Prediction.PREDICTOR.Source_Model.prediction_toplevel import KPDataset,get_data_from_dataloader_60\n",
    "import yaml\n",
    "from Training_Prediction.FOMM.Source_Model.modules.generator import OcclusionAwareGenerator\n",
    "from Training_Prediction.FOMM.Source_Model.modules.keypoint_detector import KPDetector\n",
    "from Training_Prediction.FOMM.Source_Model.logger import Logger, Visualizer, Visualizer_slow\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9050fe1a-6b18-42f6-b9c5-c797ab8fe3ee",
   "metadata": {},
   "source": [
    "# After VRNN is trained, load the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631fc218-f893-4e25-a74a-15333bfa0666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = build_vrnn\n",
    "# frames = 24\n",
    "# cfg = get_config()\n",
    "# input_keypoint = tf.keras.Input(shape=[frames,10,6],name='keypoint')\n",
    "# observed_keypoints_stop = tf.keras.layers.Lambda(tf.stop_gradient)(\n",
    "# input_keypoint)\n",
    "# vrnn_model = model(cfg)\n",
    "# predicted_keypoints, kl_divergence = vrnn_model(observed_keypoints_stop)\n",
    "# train_model = tf.keras.Model(inputs=[input_keypoint],outputs=[predicted_keypoints])\n",
    "# vrnn_coord_pred_loss = tf.nn.l2_loss(\n",
    "# observed_keypoints_stop - predicted_keypoints)\n",
    "# # Normalize by batch size and sequence length:\n",
    "# vrnn_coord_pred_loss /= tf.to_float(\n",
    "#   tf.shape(input_keypoint)[0] * tf.shape(input_keypoint)[1])\n",
    "# train_model.add_loss(vrnn_coord_pred_loss)\n",
    "# kl_loss = tf.reduce_mean(kl_divergence)  # Mean over batch and timesteps.\n",
    "# train_model.add_loss(cfg.kl_loss_scale * kl_loss)\n",
    "\n",
    "# # Load saved model:\n",
    "# cfg = get_config()\n",
    "# checkpoint_path = \"Checkpoints/VRNN_3883videos_vox_12-12.ckpt\" #  12-12 frames\n",
    "# # checkpoint_path = \"Checkpoints/VRNN_3883videos_vox_6-6.ckpt\" #  6-6 frames\n",
    "\n",
    "# # Loads the weights\n",
    "# train_model.load_weights(checkpoint_path)\n",
    "# train_model.reset_states()\n",
    "cfg = get_config()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "checkpoint_path = \"Checkpoints_VRNN/VRNN_3883videos_vox_12-12_pytorch_T3.pth\"\n",
    "model = VRNN(cfg).to(device)\n",
    "model.load_state_dict(torch.load(checkpoint_path))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde18681",
   "metadata": {},
   "source": [
    "# Import keypoints of 44 VoxCeleb driving test videos and convert to dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d3647d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"kp_test_44_vox.pkl\", \"rb\") as f:\n",
    "    kp_time_series = pickle.load(f)\n",
    "print(len(kp_time_series))\n",
    "\n",
    "#convert list of keypoints to dictionary:\n",
    "for video_idx in range(len(kp_time_series)):\n",
    "    kp_time_series[video_idx] = kp_time_series[video_idx]['kp']\n",
    "\n",
    "kp_dict_init = []\n",
    "for video_idx in range(len(kp_time_series)): # \n",
    "    init_mean = []\n",
    "    init_jacobian = []\n",
    "    for frame_idx in range(len(kp_time_series[video_idx])):\n",
    "        kp_mean = kp_time_series[video_idx][frame_idx]['value'].reshape(1,10,2)\n",
    "        kp_mean = torch.tensor(kp_mean)\n",
    "        kp_jacobian = kp_time_series[video_idx][frame_idx]['jacobian'].reshape(1,10,2,2)\n",
    "        kp_jacobian = torch.tensor(kp_jacobian)\n",
    "\n",
    "        init_mean.append(kp_mean)\n",
    "        init_jacobian.append(kp_jacobian)\n",
    "\n",
    "    init_mean = torch.cat(init_mean)\n",
    "    init_jacobian = torch.cat(init_jacobian)\n",
    "\n",
    "    init_mean = torch.reshape(init_mean,(1,init_mean.shape[0],init_mean.shape[1],init_mean.shape[2]))\n",
    "    init_jacobian = torch.reshape(init_jacobian,(1,init_jacobian.shape[0],10,2,2))\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        # add tensor to cuda\n",
    "        init_mean = init_mean.to('cuda:0')\n",
    "        init_jacobian = init_jacobian.to('cuda:0')\n",
    "\n",
    "    kp_dict_both = {\"value\":init_mean,\"jacobian\":init_jacobian}\n",
    "    kp_dict_init.append(kp_dict_both)\n",
    "    \n",
    "kp_list_test1 = []\n",
    "for video_idx in range(len(kp_dict_init)):\n",
    "    kp_one_video = torch.cat((kp_dict_init[video_idx]['value'], kp_dict_init[video_idx]['jacobian'].reshape(1,-1,10,4)),dim=-1).reshape(-1,60)\n",
    "    kp_one_video_array = np.array(kp_one_video.cpu())\n",
    "    kp_list_test1.append(kp_one_video_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a540f7d",
   "metadata": {},
   "source": [
    "# Import keypoints of 44 VoxCeleb source test videos and convert to dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25aa6e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"kp_test_44_vox_source.pkl\", \"rb\") as f:\n",
    "    kp_time_series = pickle.load(f)\n",
    "print(len(kp_time_series))\n",
    "\n",
    "#convert list of keypoints to dictionary:\n",
    "for video_idx in range(len(kp_time_series)):\n",
    "    kp_time_series[video_idx] = kp_time_series[video_idx]['kp']\n",
    "\n",
    "kp_dict_init = []\n",
    "for video_idx in range(len(kp_time_series)): # \n",
    "    init_mean = []\n",
    "    init_jacobian = []\n",
    "    for frame_idx in range(len(kp_time_series[video_idx])):\n",
    "        kp_mean = kp_time_series[video_idx][frame_idx]['value'].reshape(1,10,2)\n",
    "        kp_mean = torch.tensor(kp_mean)\n",
    "        kp_jacobian = kp_time_series[video_idx][frame_idx]['jacobian'].reshape(1,10,2,2)\n",
    "        kp_jacobian = torch.tensor(kp_jacobian)\n",
    "\n",
    "        init_mean.append(kp_mean)\n",
    "        init_jacobian.append(kp_jacobian)\n",
    "\n",
    "    init_mean = torch.cat(init_mean)\n",
    "    init_jacobian = torch.cat(init_jacobian)\n",
    "\n",
    "    init_mean = torch.reshape(init_mean,(1,init_mean.shape[0],init_mean.shape[1],init_mean.shape[2]))\n",
    "    init_jacobian = torch.reshape(init_jacobian,(1,init_jacobian.shape[0],10,2,2))\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        # add tensor to cuda\n",
    "        init_mean = init_mean.to('cuda:0')\n",
    "        init_jacobian = init_jacobian.to('cuda:0')\n",
    "\n",
    "    kp_dict_both = {\"value\":init_mean,\"jacobian\":init_jacobian}\n",
    "    kp_dict_init.append(kp_dict_both)\n",
    "    \n",
    "kp_list_test2 = []\n",
    "for video_idx in range(len(kp_dict_init)):\n",
    "    kp_one_video = torch.cat((kp_dict_init[video_idx]['value'], kp_dict_init[video_idx]['jacobian'].reshape(1,-1,10,4)),dim=-1).reshape(-1,60)\n",
    "    kp_one_video_array = np.array(kp_one_video.cpu())\n",
    "    kp_list_test2.append(kp_one_video_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f9edd0",
   "metadata": {},
   "source": [
    "# Apply min-max std to keypoints of 44 driving test videos and convert to mini-batches: 12 or 24 frames per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f49ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####  min-max std to 60 dimensions of selected one video ######\n",
    "kp_list_test_std = []\n",
    "min_list = []\n",
    "range_list = []\n",
    "for video_idx in range(len(kp_list_test1)):\n",
    "    data = kp_list_test1[video_idx]\n",
    "    data_length = len(kp_list_test1[video_idx])\n",
    "    step_interval = 12 # choose between 12 frames or 24 frames \n",
    "    min_required_steps = 2*step_interval\n",
    "    selected_data = []\n",
    "    for i in range(0, data_length - min_required_steps+1, 2 * step_interval):\n",
    "        selected_data.extend(data[i:i + step_interval])\n",
    "    min_values = np.min(selected_data,axis=0) # 60 mins of one selected video in the loop\n",
    "    max_values = np.max(selected_data,axis=0) # 60 maxs of one selected video in the loop \n",
    "    range_values = max_values - min_values \n",
    "    kp_one_video_std = (kp_list_test1[video_idx] - min_values) / range_values\n",
    "    kp_list_test_std.append(kp_one_video_std)\n",
    "    min_list.append(min_values)\n",
    "    range_list.append(range_values)\n",
    "\n",
    "trajs = kp_list_test_std\n",
    "print(len(trajs))\n",
    "print(trajs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd21ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### convert into batches:\n",
    "frames = min_required_steps\n",
    "input_frames = int(frames / 2)\n",
    "data_batch_test = []\n",
    "for t,x in enumerate(kp_list_test_std):\n",
    "    if x.shape[0] >= frames:\n",
    "        num_full_batches = x.shape[0] // frames\n",
    "        for arr in np.array_split(x[:num_full_batches * frames], num_full_batches):\n",
    "            data_batch_test.append(arr)\n",
    "print(f'test dataset batches:', len(data_batch_test))\n",
    "print(data_batch_test[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9960c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### test dataset:\n",
    "\n",
    "test_data_reshape = np.array(data_batch_test).reshape(-1,frames,60)\n",
    "test_data_reshape.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376c19e5-ee36-44cc-a8c7-7caff90e7d29",
   "metadata": {},
   "source": [
    "# Predict keypoints using trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c9c688-c031-4a26-8a24-2720d2b21e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test dataset and process model.predict():\n",
    "\n",
    "# validation_data = test_data_reshape\n",
    "\n",
    "# validation_data_tensor = tf.convert_to_tensor(validation_data.reshape(-1,frames,10,6))\n",
    "# pred = train_model.predict(validation_data_tensor)\n",
    "# print(pred.shape)\n",
    "validation_data = test_data_reshape.reshape(-1, frames, 10, 6)\n",
    "validation_data_tensor = torch.tensor(validation_data, dtype=torch.float32).to(device)\n",
    "\n",
    "# run inference\n",
    "with torch.no_grad():\n",
    "    pred, kl_div = model(validation_data_tensor)\n",
    "\n",
    "print(pred.shape)  # should be [B, T, 10, 6]\n",
    "preds = pred.mean(axis=1)\n",
    "pred_np = preds.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19b86c4-b761-48d9-ad9a-88908129633b",
   "metadata": {},
   "source": [
    "# Generate unstd keypoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d67c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save num_batches for each video:\n",
    "num_batch_video = []\n",
    "num_full_batches_all = 0\n",
    "for t,x in enumerate(kp_list_test_std):\n",
    "    if x.shape[0] > frames:\n",
    "        num_full_batches = x.shape[0] // frames\n",
    "        num_full_batches_all += num_full_batches\n",
    "        num_batch_video.append(num_full_batches)\n",
    "print(f'number of batches of each video:', len(num_batch_video))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacf886e-0203-4086-bedb-4468e1cf4fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first half of frames: groundtruth; last half of frames: predicted\n",
    "test_gt_pred = np.concatenate((test_data_reshape[:,:input_frames], pred_np.reshape(-1,frames,60)[:,input_frames:]), axis = 1)\n",
    "test_gt_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc63c71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unstd for each video:\n",
    "test_video_unstd_list = []\n",
    "for video_idx in range(len(num_batch_video)):\n",
    "    test_video = test_gt_pred[sum(num_batch_video[:video_idx]):sum(num_batch_video[:video_idx+1])]\n",
    "    test_video_unstd = test_video * range_list[video_idx] + min_list[video_idx]\n",
    "    test_video_unstd_list.append(test_video_unstd) # unstd video keypoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b74a397-20c6-4395-b7b9-c14de9621770",
   "metadata": {},
   "source": [
    "# Optical flow and generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbe6fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### call the config functions and inference dataloader #########\n",
    "\n",
    "config=\"config/abs-vox.yml\"\n",
    "\n",
    "# Test dataset\n",
    "with open(config) as f:\n",
    "    config = yaml.safe_load(f)\n",
    "dataset1 = FramesDataset_transfer(is_train=(False), **config['dataset_params'],mode=\"RNN\") # test: driving videos\n",
    "print(len(dataset1))\n",
    "dataloader1 = DataLoader(dataset1, batch_size=1, shuffle=False, num_workers=1)\n",
    "\n",
    "dataset2 = FramesDataset_transfer(is_train=(True), **config['dataset_params'],mode=\"VRNN\") # test_source: source videos\n",
    "print(len(dataset2))\n",
    "dataloader2 = DataLoader(dataset2, batch_size=1, shuffle=False, num_workers=1)\n",
    "\n",
    "dataset3 = FramesDataset_transfer(is_train=(False), **config['dataset_params'],mode=\"VRNN\") # test_recon: reference videos(only FOMM, no keypoints prediction)\n",
    "print(len(dataset3))\n",
    "dataloader3 = DataLoader(dataset3, batch_size=1, shuffle=False, num_workers=1)\n",
    "\n",
    "### call the functions        \n",
    "generator = OcclusionAwareGenerator(**config['model_params']['generator_params'],\n",
    "                                        **config['model_params']['common_params'])\n",
    "kp_detector = KPDetector(**config['model_params']['kp_detector_params'],\n",
    "                            **config['model_params']['common_params'])\n",
    "\n",
    "log_dir=\"./log/test-reconstruction-vox\"\n",
    "checkpoint=\"./Training_Prediction/FOMM/Trained_Models/vox-cpk.pth.tar\"\n",
    "\n",
    "if checkpoint is not None:\n",
    "    Logger.load_cpk(checkpoint, generator=generator, kp_detector=kp_detector)\n",
    "else:\n",
    "    raise AttributeError(\"Checkpoint should be specified for mode='reconstruction'.\")\n",
    "    \n",
    "def save_obj(obj, name ):\n",
    "    with open('./'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('./' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "png_dir = os.path.join(log_dir, 'prediction/png')\n",
    "log_dir = os.path.join(log_dir, 'prediction')\n",
    "\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "if not os.path.exists(png_dir):\n",
    "    os.makedirs(png_dir)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    generator = DataParallelWithCallback(generator)\n",
    "    kp_detector = DataParallelWithCallback(kp_detector)\n",
    "\n",
    "generator.eval()\n",
    "kp_detector.eval()\n",
    "\n",
    "prediction_params = config['prediction_params']\n",
    "\n",
    "num_epochs = prediction_params['num_epochs']\n",
    "lr = prediction_params['lr']\n",
    "bs = prediction_params['batch_size']\n",
    "num_frames = prediction_params['num_frames']\n",
    "loss_list_total = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deda397f-1e95-4317-87fb-ae5830ec77ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for it, x in tqdm(enumerate(dataloader1)):\n",
    "        if config['reconstruction_params']['num_videos'] is not None:\n",
    "            if it > config['reconstruction_params']['num_videos']:\n",
    "                break\n",
    "        with torch.no_grad():\n",
    "            predictions = []\n",
    "            visualizations = []\n",
    "\n",
    "            ######## keypoints ########\n",
    "            kp_driving_video = test_video_unstd_list[it].reshape(-1,10,6) \n",
    "            kp_driving_video = torch.tensor(kp_driving_video).to(device)\n",
    "            kp_source = {\"value\":torch.tensor(kp_list_test2[it][0]).reshape(10,6)[:,:2].to(device),\"jacobian\":torch.tensor(kp_list_test2[it][0]).reshape(10,6)[:,2:].reshape(1,10,2,2).to(device)} # kp of the ith frame \n",
    "            generator = generator.to(device)\n",
    "        \n",
    "        ##### Start generator\n",
    "        loss_list = []\n",
    "        for i in range(((x['video'].shape[2])//frames)*frames): # cut the last <24 frames\n",
    "            driving = x['video'][:, :, i].to(device)\n",
    "            source = torch.tensor(dataset2[it]['video'][:,0]).reshape(1,3,256,256).to(device) # source frame from set2 \n",
    "            driving_reference = torch.tensor(dataset3[it]['video'][:,i]).reshape(1,3,256,256)\n",
    "            kp_driving = {\"value\":kp_driving_video[i,:,:2],\"jacobian\":kp_driving_video[i,:,2:]} # kp of the ith frame\n",
    "            kp_driving['value'] = kp_driving['value'].reshape(1,10,2)\n",
    "            kp_driving['jacobian'] = kp_driving['jacobian'].reshape(1,10,2,2)\n",
    "            out = generator(source, kp_source=kp_source, kp_driving=kp_driving)\n",
    "            out['kp_source'] = kp_source\n",
    "            out['kp_driving'] = kp_driving\n",
    "            del out['sparse_deformed']\n",
    "            predictions.append(np.transpose(out['prediction'].data.cpu().numpy(), [0, 2, 3, 1])[0])\n",
    "\n",
    "            visualization = Visualizer(**config['visualizer_params']).visualize(source=source,\n",
    "                                                                                    driving=driving, out=out)\n",
    "            visualizations.append(visualization) #visualizations[0].shape: (256, 1280, 3)\n",
    "            # mqe loss\n",
    "            if np.abs(out['prediction'].detach().cpu().numpy() - driving_reference.cpu().numpy()).mean() != 0:\n",
    "                loss_list.append(np.abs(out['prediction'].detach().cpu().numpy() - driving_reference.cpu().numpy()).mean())\n",
    "            del driving, source, driving_reference, kp_driving, out, visualization\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        print(\"Reconstruction loss: %s\" % np.mean(loss_list))\n",
    "        loss_list_total.append(np.mean(loss_list))\n",
    "        \n",
    "        predictions = np.concatenate(predictions, axis=1)\n",
    "        imageio.imsave(os.path.join(png_dir, x['name'][0] + '.png'), (255 * predictions).astype(np.uint8))\n",
    "        image_name = x['name'][0] + config['reconstruction_params']['format']\n",
    "        imageio.mimsave(os.path.join(log_dir, image_name), visualizations)\n",
    "        del predictions, visualizations\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(\"mean Reconstruction loss: %s\" % np.mean(loss_list_total)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

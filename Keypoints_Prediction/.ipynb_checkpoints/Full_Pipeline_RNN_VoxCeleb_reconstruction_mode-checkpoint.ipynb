{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09e5a098",
   "metadata": {},
   "source": [
    "# This is a full pipeline with keypoints prediction using RNN in reconstruction mode for VoxCeleb dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4af7e9",
   "metadata": {},
   "source": [
    "# Import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa4a0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os   \n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from Training_Prediction.FOMM.Source_Model.logger import Logger, Visualizer\n",
    "import numpy as np\n",
    "import imageio\n",
    "from Training_Prediction.FOMM.Source_Model.sync_batchnorm import DataParallelWithCallback\n",
    "from Training_Prediction.FOMM.Source_Model.modules.RNN_prediction_module import PredictionModule\n",
    "from Training_Prediction.FOMM.Source_Model.augmentation import SelectRandomFrames, SelectFirstFrames_two, VideoToTensor\n",
    "from tqdm import trange\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from Training_Prediction.FOMM.Source_Model.frames_dataset import FramesDataset\n",
    "import tensorflow.compat.v1 as tf\n",
    "import pickle\n",
    "from Training_Prediction.PREDICTOR.Source_Model.prediction_toplevel import KPDataset,get_data_from_dataloader_60\n",
    "import gc\n",
    "import pickle\n",
    "import yaml\n",
    "from Training_Prediction.FOMM.Source_Model.modules.generator import OcclusionAwareGenerator,calculate_frechet_distance,compute_fvd\n",
    "from Training_Prediction.FOMM.Source_Model.modules.keypoint_detector import KPDetector\n",
    "from Training_Prediction.FOMM.Source_Model.logger import Logger, Visualizer, Visualizer_slow\n",
    "from torch import nn\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "from RNN import GRUModel\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import os, sys\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47951536",
   "metadata": {},
   "source": [
    "# Import keypoints of 3883 training videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaa8d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"kp_train_3883_vox.pkl\", \"rb\") as f:\n",
    "    kp_time_series = pickle.load(f)\n",
    "len(kp_time_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881775e8",
   "metadata": {},
   "source": [
    "# Convert list of keypoints to dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49eaf979",
   "metadata": {},
   "outputs": [],
   "source": [
    "for video_idx in range(len(kp_time_series)):\n",
    "    kp_time_series[video_idx] = kp_time_series[video_idx]['kp']\n",
    "\n",
    "kp_dict_init = []\n",
    "for video_idx in range(len(kp_time_series)): \n",
    "    init_mean = []\n",
    "    init_jacobian = []\n",
    "    for frame_idx in range(len(kp_time_series[video_idx])):\n",
    "        kp_mean = kp_time_series[video_idx][frame_idx]['value'].reshape(1,10,2)\n",
    "        kp_mean = torch.tensor(kp_mean)\n",
    "        kp_jacobian = kp_time_series[video_idx][frame_idx]['jacobian'].reshape(1,10,2,2)\n",
    "        kp_jacobian = torch.tensor(kp_jacobian)\n",
    "\n",
    "        init_mean.append(kp_mean)\n",
    "        init_jacobian.append(kp_jacobian)\n",
    "\n",
    "    init_mean = torch.cat(init_mean)\n",
    "    init_jacobian = torch.cat(init_jacobian)\n",
    "\n",
    "    init_mean = torch.reshape(init_mean,(1,init_mean.shape[0],init_mean.shape[1],init_mean.shape[2]))\n",
    "    init_jacobian = torch.reshape(init_jacobian,(1,init_jacobian.shape[0],10,2,2))\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        # add tensor to cuda\n",
    "        init_mean = init_mean.to('cuda:0')\n",
    "        init_jacobian = init_jacobian.to('cuda:0')\n",
    "\n",
    "    kp_dict_both = {\"value\":init_mean,\"jacobian\":init_jacobian}\n",
    "    kp_dict_init.append(kp_dict_both)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f2996e",
   "metadata": {},
   "source": [
    "# Apply min-max standardization to keypoints "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e9981e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kp_list_train = []\n",
    "for video_idx in range(len(kp_dict_init)):\n",
    "    kp_one_video = torch.cat((kp_dict_init[video_idx]['value'], kp_dict_init[video_idx]['jacobian'].reshape(1,-1,10,4)),dim=-1).reshape(-1,60)\n",
    "    kp_one_video_array = np.array(kp_one_video.cpu())\n",
    "    kp_list_train.append(kp_one_video_array)\n",
    "    \n",
    "#####  min-max std to 60 dimensions of selected one video ######\n",
    "kp_list_train_std = []\n",
    "min_list = []\n",
    "range_list = []\n",
    "for video_idx in range(len(kp_list_train)):\n",
    "    min_values = np.min(kp_list_train[video_idx],axis=0) # 60 mins of one selected video in the loop\n",
    "    max_values = np.max(kp_list_train[video_idx],axis=0) # 60 maxs of one selected video in the loop\n",
    "    range_values = max_values - min_values \n",
    "    kp_one_video_std = (kp_list_train[video_idx] - min_values) / range_values\n",
    "    kp_list_train_std.append(kp_one_video_std)\n",
    "    min_list.append(min_values)\n",
    "    range_list.append(range_values)\n",
    "\n",
    "trajs = kp_list_train_std\n",
    "print(len(trajs))\n",
    "print(trajs[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbac9895",
   "metadata": {},
   "source": [
    "# Convert standardized keypoints to mini-batches: 12 or 24 frames a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e41ef71",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### convert data into batches #########\n",
    "data_batch_train = []\n",
    "\n",
    "frames = 24 # 24 as one batch, use 12 ground truth frames as input to predict next 12 frames as output\n",
    "input_frames = int(frames / 2) \n",
    "input_dim = 60\n",
    "for t,x in enumerate(kp_list_train_std):\n",
    "    if x.shape[0] > frames:\n",
    "        num_full_batches = x.shape[0] // frames\n",
    "        for arr in np.array_split(x[:num_full_batches * frames], num_full_batches):\n",
    "            data_batch_train.append(arr)\n",
    "print(f'train dataset batches:', len(data_batch_train))\n",
    "print(data_batch_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bce9fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### train dataset:\n",
    "\n",
    "train_data_reshape = np.array(data_batch_train).reshape(-1,frames,60)\n",
    "train_data_reshape.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5e6305",
   "metadata": {},
   "source": [
    "# Define RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0265767c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model with dynamic teacher forcing probability\n",
    "input_dim = 60\n",
    "hidden_dim = 256\n",
    "output_dim = input_dim\n",
    "num_layers = 3\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "model = GRUModel(input_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab338ad",
   "metadata": {},
   "source": [
    "# dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa55421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model \n",
    "train_data = train_data_reshape\n",
    "\n",
    "# Convert train_data to DataLoader for efficient batching and shuffling\n",
    "train_loader = DataLoader(train_data, batch_size=250, shuffle=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for step, batch in enumerate(train_loader):\n",
    "\n",
    "        batch_input = batch[:,:input_frames]\n",
    "        batch_output = batch[:,input_frames:]\n",
    "        outputs = model(batch_input)\n",
    "\n",
    "        loss = criterion(outputs, batch_output)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Step {step + 1}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
    "    \n",
    "# Save the best model's state_dict to a file\n",
    "torch.save(model.state_dict(), 'RNN_3883videos_vox_12-12.pth') # 12-12 frames\n",
    "# torch.save(model.state_dict(), 'RNN_3883videos_vox_6-6.pth') # 6-6 frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f25b2a",
   "metadata": {},
   "source": [
    "# After RNN is trained, load saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8454dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved parameters\n",
    "model.load_state_dict(torch.load('RNN_3883videos_vox_12-12.pth')) # 12-12 frames\n",
    "# model.load_state_dict(torch.load('RNN_3883videos_vox_6-6.pth')) # 6-6 frames\n",
    "\n",
    "# Set the model to evaluation mode (important if using dropout or batch normalization)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc109de",
   "metadata": {},
   "source": [
    "# Import keypoints of 44 VoxCeleb test videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3df2e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"kp_test_44_vox.pkl\", \"rb\") as f:\n",
    "    kp_time_series = pickle.load(f)\n",
    "len(kp_time_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56134b02",
   "metadata": {},
   "source": [
    "# Convert list of keypoints to dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95d8b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "for video_idx in range(len(kp_time_series)):\n",
    "    kp_time_series[video_idx] = kp_time_series[video_idx]['kp']\n",
    "\n",
    "kp_dict_init = []\n",
    "for video_idx in range(len(kp_time_series)): # \n",
    "    init_mean = []\n",
    "    init_jacobian = []\n",
    "    for frame_idx in range(len(kp_time_series[video_idx])):\n",
    "        kp_mean = kp_time_series[video_idx][frame_idx]['value'].reshape(1,10,2)\n",
    "        kp_mean = torch.tensor(kp_mean)\n",
    "        kp_jacobian = kp_time_series[video_idx][frame_idx]['jacobian'].reshape(1,10,2,2)\n",
    "        kp_jacobian = torch.tensor(kp_jacobian)\n",
    "\n",
    "        init_mean.append(kp_mean)\n",
    "        init_jacobian.append(kp_jacobian)\n",
    "\n",
    "    init_mean = torch.cat(init_mean)\n",
    "    init_jacobian = torch.cat(init_jacobian)\n",
    "\n",
    "    init_mean = torch.reshape(init_mean,(1,init_mean.shape[0],init_mean.shape[1],init_mean.shape[2]))\n",
    "    init_jacobian = torch.reshape(init_jacobian,(1,init_jacobian.shape[0],10,2,2))\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        # add tensor to cuda\n",
    "        init_mean = init_mean.to('cuda:0')\n",
    "        init_jacobian = init_jacobian.to('cuda:0')\n",
    "\n",
    "    kp_dict_both = {\"value\":init_mean,\"jacobian\":init_jacobian}\n",
    "    kp_dict_init.append(kp_dict_both)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67981bd7",
   "metadata": {},
   "source": [
    "# Apply min-max std to keypoints and convert to batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c9cfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "kp_list_test = []\n",
    "for video_idx in range(len(kp_dict_init)):\n",
    "    kp_one_video = torch.cat((kp_dict_init[video_idx]['value'], kp_dict_init[video_idx]['jacobian'].reshape(1,-1,10,4)),dim=-1).reshape(-1,60)\n",
    "    kp_one_video_array = np.array(kp_one_video.cpu())\n",
    "    kp_list_test.append(kp_one_video_array)\n",
    "    \n",
    "#####  min-max std to 60 dimensions of selected one video ######\n",
    "kp_list_test_std = []\n",
    "min_list = []\n",
    "range_list = []\n",
    "for video_idx in range(len(kp_list_test)):\n",
    "    min_values = np.min(kp_list_test[video_idx],axis=0) # 60 mins of one selected video in the loop\n",
    "    max_values = np.max(kp_list_test[video_idx],axis=0) # 60 maxs of one selected video in the loop\n",
    "    range_values = max_values - min_values \n",
    "    kp_one_video_std = (kp_list_test[video_idx] - min_values) / range_values\n",
    "    kp_list_test_std.append(kp_one_video_std)\n",
    "    min_list.append(min_values)\n",
    "    range_list.append(range_values)\n",
    "\n",
    "trajs = kp_list_test_std\n",
    "print(len(trajs))\n",
    "print(trajs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b33cd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### convert into batches:\n",
    "frames = 24 # 24 frames or 12 frames\n",
    "input_frames = int(frames / 2)\n",
    "data_batch_test = []\n",
    "for t,x in enumerate(kp_list_test_std):\n",
    "    if x.shape[0] >= frames:\n",
    "        num_full_batches = x.shape[0] // frames\n",
    "        for arr in np.array_split(x[:num_full_batches * frames], num_full_batches):\n",
    "            data_batch_test.append(arr)\n",
    "print(f'test dataset batches:', len(data_batch_test))\n",
    "print(data_batch_test[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cf44c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### test dataset:\n",
    "\n",
    "test_data_reshape = np.array(data_batch_test).reshape(-1,frames,60)\n",
    "test_data_reshape.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376c19e5-ee36-44cc-a8c7-7caff90e7d29",
   "metadata": {},
   "source": [
    "# Predict keypoints using trained model and check std keypoints MSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c9c688-c031-4a26-8a24-2720d2b21e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dataset\n",
    "validation_data = test_data_reshape\n",
    "\n",
    "# evaluate model:\n",
    "validation_input = torch.tensor(validation_data[:,:input_frames], dtype = torch.float32) # input: [24,10,17]\n",
    "kp_gt = torch.tensor(validation_data[:,input_frames:], dtype = torch.float32) # gtoundtruth: [24,10,17]\n",
    "pred = model(validation_input) # outputs: [24,10,30]\n",
    "\n",
    "print(kp_gt.shape)\n",
    "print(pred.shape)\n",
    "\n",
    "# MSE loss of std data:\n",
    "mse = np.mean((np.array(pred.detach().numpy())- np.array(kp_gt))**2)\n",
    "rounded_mse = round(mse, 4)\n",
    "print(f'MSE: ', rounded_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19b86c4-b761-48d9-ad9a-88908129633b",
   "metadata": {},
   "source": [
    "# Check unstd keypoints MSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd71d1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save num_batches for each video:\n",
    "num_batch_video = []\n",
    "num_full_batches_all = 0\n",
    "for t,x in enumerate(kp_list_test_std):\n",
    "    if x.shape[0] > frames:\n",
    "        num_full_batches = x.shape[0] // frames\n",
    "        num_full_batches_all += num_full_batches\n",
    "        num_batch_video.append(num_full_batches)\n",
    "print(f'number of batches of each video:', len(num_batch_video))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacf886e-0203-4086-bedb-4468e1cf4fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first half of frames: groundtruth; last half of frames: predicted\n",
    "test_gt_pred = np.concatenate((test_data_reshape[:,:input_frames], pred.detach().numpy()), axis = 1)\n",
    "test_gt_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56be0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unstd for each video:\n",
    "test_video_unstd_list = []\n",
    "for video_idx in range(len(num_batch_video)):\n",
    "    test_video = test_gt_pred[sum(num_batch_video[:video_idx]):sum(num_batch_video[:video_idx+1])]\n",
    "    test_video_unstd = test_video * range_list[video_idx] + min_list[video_idx]\n",
    "    test_video_unstd_list.append(test_video_unstd) # unstd video keypoints\n",
    "# unstd for each video of groundtruth data for MSE loss with std:\n",
    "validation_gt_unstd_list = []\n",
    "for video_idx in range(len(num_batch_video)):\n",
    "    validation_gt = validation_data[sum(num_batch_video[:video_idx]):sum(num_batch_video[:video_idx+1])]\n",
    "    validation_gt_unstd = validation_gt * range_list[video_idx] + min_list[video_idx]\n",
    "    validation_gt_unstd_list.append(validation_gt_unstd) # unstd video keypoints "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d8bbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE loss of unstd test data:\n",
    "validation_gt_unstd_array = np.concatenate(validation_gt_unstd_list, axis=0)\n",
    "print(validation_gt_unstd_array[:,input_frames:].shape) # last 12 gt frames \n",
    "\n",
    "test_video_unstd_array = np.concatenate(test_video_unstd_list, axis=0)\n",
    "print(test_video_unstd_array[:,input_frames:].shape) # last 12 pred frames\n",
    "\n",
    "mse = np.mean((test_video_unstd_array[:,input_frames:]- validation_gt_unstd_array[:,input_frames:])**2)\n",
    "rounded_mse = round(mse, 4)\n",
    "print(f'MSE: ', rounded_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b74a397-20c6-4395-b7b9-c14de9621770",
   "metadata": {},
   "source": [
    "# Optical flow and generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c8b2f0-07df-4c33-8293-af8dbd119772",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### call the config functions and inference dataloader #########\n",
    "config=\"/home/jovyan/srinjoy-gan-vol/keypoints_project/config/abs-vox.yml\"\n",
    "\n",
    "# Test dataset\n",
    "with open(config) as f:\n",
    "    config = yaml.safe_load(f)\n",
    "dataset = FramesDataset(is_train=(False), **config['dataset_params'],mode=\"RNN\") # test\n",
    "\n",
    "print(len(dataset))\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=1)\n",
    "\n",
    "### call the functions        \n",
    "generator = OcclusionAwareGenerator(**config['model_params']['generator_params'],\n",
    "                                        **config['model_params']['common_params'])\n",
    "kp_detector = KPDetector(**config['model_params']['kp_detector_params'],\n",
    "                            **config['model_params']['common_params'])\n",
    "\n",
    "log_dir=\"/home/jovyan/srinjoy-gan-vol/keypoints_project/log/test-reconstruction-vox\"\n",
    "checkpoint=\"/home/jovyan/srinjoy-gan-vol/keypoints_project/Training_Prediction/FOMM/Trained_Models/vox-cpk.pth.tar\"\n",
    "\n",
    "if checkpoint is not None:\n",
    "    Logger.load_cpk(checkpoint, generator=generator, kp_detector=kp_detector)\n",
    "else:\n",
    "    raise AttributeError(\"Checkpoint should be specified for mode='reconstruction'.\")\n",
    "    \n",
    "def save_obj(obj, name ):\n",
    "    with open('./'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('./' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "png_dir = os.path.join(log_dir, 'prediction/png')\n",
    "log_dir = os.path.join(log_dir, 'prediction')\n",
    "\n",
    "if checkpoint is not None:\n",
    "    Logger.load_cpk(checkpoint, generator=generator, kp_detector=kp_detector)\n",
    "else:\n",
    "    raise AttributeError(\"Checkpoint should be specified for mode='reconstruction'.\")\n",
    "    \n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "if not os.path.exists(png_dir):\n",
    "    os.makedirs(png_dir)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    generator = DataParallelWithCallback(generator)\n",
    "    kp_detector = DataParallelWithCallback(kp_detector)\n",
    "\n",
    "generator = DataParallelWithCallback(generator)\n",
    "kp_detector = DataParallelWithCallback(kp_detector)\n",
    "\n",
    "generator.eval()\n",
    "kp_detector.eval()\n",
    "\n",
    "prediction_params = config['prediction_params']\n",
    "\n",
    "num_epochs = prediction_params['num_epochs']\n",
    "lr = prediction_params['lr']\n",
    "bs = prediction_params['batch_size']\n",
    "num_frames = prediction_params['num_frames']\n",
    "loss_list_total = []\n",
    "fvd_list_total = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a2bd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########  FOMM+RNN ########\n",
    "\n",
    "for it, x in tqdm(enumerate(dataloader)):\n",
    "        if config['reconstruction_params']['num_videos'] is not None:\n",
    "            if it > config['reconstruction_params']['num_videos']:\n",
    "                break\n",
    "        with torch.no_grad():\n",
    "            predictions = []\n",
    "            visualizations = []\n",
    "\n",
    "            ######## keypoints ########\n",
    "            kp_driving_video = test_video_unstd_list[it].reshape(-1,10,6)\n",
    "            kp_driving_video = torch.tensor(kp_driving_video)\n",
    "            kp_source = {\"value\":kp_driving_video[0,:,:2].reshape(1,10,2),\"jacobian\":kp_driving_video[0,:,2:].reshape(1,10,2,2)} # kp of the ith frame      \n",
    "        \n",
    "        ##### Start generator\n",
    "        loss_list = []\n",
    "        fvd_list = []\n",
    "        for i in range(((x['video'].shape[2])//frames)*frames): # cut the last <24 frames\n",
    "            source = x['video'][:, :, 0]\n",
    "            driving = x['video'][:, :, i]\n",
    "            kp_driving = {\"value\":kp_driving_video[i,:,:2],\"jacobian\":kp_driving_video[i,:,2:]} # kp of the ith frame\n",
    "            kp_driving['value'] = kp_driving['value'].reshape(1,10,2)\n",
    "            kp_driving['jacobian'] = kp_driving['jacobian'].reshape(1,10,2,2)\n",
    "            out = generator(source, kp_source=kp_source, kp_driving=kp_driving)\n",
    "            out['kp_source'] = kp_source\n",
    "            out['kp_driving'] = kp_driving\n",
    "            del out['sparse_deformed']\n",
    "            predictions.append(np.transpose(out['prediction'].data.cpu().numpy(), [0, 2, 3, 1])[0])\n",
    "\n",
    "            visualization = Visualizer(**config['visualizer_params']).visualize(source=source,\n",
    "                                                                                    driving=driving, out=out)\n",
    "            visualizations.append(visualization)\n",
    "            # mse loss\n",
    "            if np.abs(out['prediction'].detach().cpu().numpy() - driving.cpu().numpy()).mean() != 0:\n",
    "                loss_list.append(np.abs(out['prediction'].detach().cpu().numpy() - driving.cpu().numpy()).mean())\n",
    "                # Calculate FVD for each frame using ground truth and predicted videos\n",
    "                ground_truth_features = driving.detach().cpu().permute(0,2,3,1).reshape(256,256,3)\n",
    "                predicted_features = out['prediction'].detach().cpu().permute(0,2,3,1).reshape(256,256,3)\n",
    "                fvd_list.append(compute_fvd(ground_truth_features, predicted_features))\n",
    "\n",
    "        print(\"Reconstruction loss: %s\" % np.mean(loss_list))\n",
    "        loss_list_total.append(np.mean(loss_list))\n",
    "\n",
    "        print(\"FVD Score: %s\" % np.mean(fvd_list))\n",
    "        fvd_list_total.append(np.mean(fvd_list))\n",
    "\n",
    "        predictions = np.concatenate(predictions, axis=1)\n",
    "        imageio.imsave(os.path.join(png_dir, x['name'][0] + '.png'), (255 * predictions).astype(np.uint8))\n",
    "        image_name = x['name'][0] + config['reconstruction_params']['format']\n",
    "        imageio.mimsave(os.path.join(log_dir, image_name), visualizations)\n",
    "\n",
    "print(\"mean Reconstruction loss: %s\" % np.mean(loss_list_total)) \n",
    "print(\"mean FVD score: %s\" % np.mean(fvd_list_total)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

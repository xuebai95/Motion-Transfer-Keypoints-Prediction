{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09e5a098",
   "metadata": {},
   "source": [
    "# This is a full pipeline with keypoints prediction using VRNN in reconstruction mode for VoxCeleb dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4af7e9",
   "metadata": {},
   "source": [
    "# Import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa4a0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os   \n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from Training_Prediction.FOMM.Source_Model.logger import Logger, Visualizer\n",
    "import numpy as np\n",
    "import imageio\n",
    "from Training_Prediction.FOMM.Source_Model.sync_batchnorm import DataParallelWithCallback\n",
    "from Training_Prediction.FOMM.Source_Model.modules.RNN_prediction_module import PredictionModule\n",
    "from Training_Prediction.FOMM.Source_Model.augmentation import SelectRandomFrames, SelectFirstFrames_two, VideoToTensor\n",
    "from tqdm import trange\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from Training_Prediction.FOMM.Source_Model.frames_dataset import FramesDataset\n",
    "import tensorflow.compat.v1 as tf\n",
    "from Training_Prediction.PREDICTOR.Source_Model.VRNN import build_vrnn, get_config\n",
    "import pickle\n",
    "from Training_Prediction.PREDICTOR.Source_Model.VRNN_prediction import VRNN_predict\n",
    "from Training_Prediction.PREDICTOR.Source_Model.prediction_toplevel import KPDataset,get_data_from_dataloader_60\n",
    "\n",
    "import gc\n",
    "import pickle\n",
    "import yaml\n",
    "from Training_Prediction.FOMM.Source_Model.modules.generator import OcclusionAwareGenerator,calculate_frechet_distance,compute_fvd\n",
    "from Training_Prediction.FOMM.Source_Model.modules.keypoint_detector import KPDetector\n",
    "from Training_Prediction.FOMM.Source_Model.logger import Logger, Visualizer, Visualizer_slow\n",
    "from torch import nn\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "import os, sys\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b914babd",
   "metadata": {},
   "source": [
    "# Import keypoints of 3883 training videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dddb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"kp_train_3883_vox.pkl\", \"rb\") as f:\n",
    "    kp_time_series = pickle.load(f)\n",
    "len(kp_time_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6229644",
   "metadata": {},
   "source": [
    "# Convert list of keypoints to dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546511ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list of keypoints to dictionary: \n",
    "for video_idx in range(len(kp_time_series)):\n",
    "    kp_time_series[video_idx] = kp_time_series[video_idx]['kp']\n",
    "\n",
    "kp_dict_init = []\n",
    "for video_idx in range(len(kp_time_series)): # \n",
    "    init_mean = []\n",
    "    init_jacobian = []\n",
    "    for frame_idx in range(len(kp_time_series[video_idx])):\n",
    "        kp_mean = kp_time_series[video_idx][frame_idx]['value'].reshape(1,10,2)\n",
    "        kp_mean = torch.tensor(kp_mean)\n",
    "        kp_jacobian = kp_time_series[video_idx][frame_idx]['jacobian'].reshape(1,10,2,2)\n",
    "        kp_jacobian = torch.tensor(kp_jacobian)\n",
    "\n",
    "        init_mean.append(kp_mean)\n",
    "        init_jacobian.append(kp_jacobian)\n",
    "\n",
    "    init_mean = torch.cat(init_mean)\n",
    "    init_jacobian = torch.cat(init_jacobian)\n",
    "\n",
    "    init_mean = torch.reshape(init_mean,(1,init_mean.shape[0],init_mean.shape[1],init_mean.shape[2]))\n",
    "    init_jacobian = torch.reshape(init_jacobian,(1,init_jacobian.shape[0],10,2,2))\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        # add tensor to cuda\n",
    "        init_mean = init_mean.to('cuda:0')\n",
    "        init_jacobian = init_jacobian.to('cuda:0')\n",
    "\n",
    "    kp_dict_both = {\"value\":init_mean,\"jacobian\":init_jacobian}\n",
    "    kp_dict_init.append(kp_dict_both)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abf8684",
   "metadata": {},
   "source": [
    "# Apply min-max standardization to keypoints "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebb966a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kp_list_train = []\n",
    "for video_idx in range(len(kp_dict_init)):\n",
    "    kp_one_video = torch.cat((kp_dict_init[video_idx]['value'], kp_dict_init[video_idx]['jacobian'].reshape(1,-1,10,4)),dim=-1).reshape(-1,60)\n",
    "    kp_one_video_array = np.array(kp_one_video.cpu())\n",
    "    kp_list_train.append(kp_one_video_array)\n",
    "    \n",
    "#####  min-max std to 60 dimensions of selected one video ######\n",
    "kp_list_train_std = []\n",
    "min_list = []\n",
    "range_list = []\n",
    "for video_idx in range(len(kp_list_train)):\n",
    "    min_values = np.min(kp_list_train[video_idx],axis=0) # 60 mins of one selected video in the loop\n",
    "    max_values = np.max(kp_list_train[video_idx],axis=0) # 60 maxs of one selected video in the loop\n",
    "    range_values = max_values - min_values \n",
    "    kp_one_video_std = (kp_list_train[video_idx] - min_values) / range_values\n",
    "    kp_list_train_std.append(kp_one_video_std)\n",
    "    min_list.append(min_values)\n",
    "    range_list.append(range_values)\n",
    "\n",
    "trajs = kp_list_train_std\n",
    "print(len(trajs))\n",
    "print(trajs[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42c77e0",
   "metadata": {},
   "source": [
    "# Convert standardized keypoints to mini-batches: 12 or 24 frames a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0edb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### convert data into batches #########\n",
    "data_batch_train = []\n",
    "\n",
    "frames = 24 # 24 as one batch, use 12 ground truth frames as input to predict next 12 frames as output\n",
    "input_frames = int(frames / 2)\n",
    "input_dim = 60\n",
    "for t,x in enumerate(kp_list_train_std):\n",
    "    if x.shape[0] >= frames:\n",
    "        num_full_batches = x.shape[0] // frames\n",
    "        for arr in np.array_split(x[:num_full_batches * frames], num_full_batches):\n",
    "            data_batch_train.append(arr)\n",
    "print(f'train dataset batches:', len(data_batch_train))\n",
    "print(data_batch_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fc174b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### train dataset:\n",
    "\n",
    "train_data_reshape = np.array(data_batch_train).reshape(-1,frames,60)\n",
    "train_data_reshape.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e345974b",
   "metadata": {},
   "source": [
    "#  Train VRNN with build_vrnn_new/build_vrnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ae4d2e-1a4e-4398-8349-d8dd6fda01aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_vrnn\n",
    "\n",
    "print(\"using VRNN\")\n",
    "train_data = train_data_reshape\n",
    "\n",
    "frames = 24\n",
    "data_all = {}\n",
    "data_all['value'] = []\n",
    "data_all['jacobian'] = []\n",
    "data_lee = []\n",
    "\n",
    "#data_lee = get_data_50(test_dataset)\n",
    "data_lee = train_data\n",
    "print(data_lee.shape)\n",
    "data_lee = np.reshape(data_lee,(-1,frames,10,6))\n",
    "print(data_lee.shape)\n",
    "data_lee = tf.convert_to_tensor(data_lee)\n",
    "print(data_lee.shape)\n",
    "cfg = get_config()\n",
    "input_keypoint = tf.keras.Input(shape=[frames,10,6],name='keypoint')\n",
    "observed_keypoints_stop = tf.keras.layers.Lambda(tf.stop_gradient)(\n",
    "input_keypoint)\n",
    "vrnn_model = model(cfg)\n",
    "predicted_keypoints, kl_divergence = vrnn_model(observed_keypoints_stop)\n",
    "train_model = tf.keras.Model(inputs=[input_keypoint],outputs=[predicted_keypoints])\n",
    "vrnn_coord_pred_loss = tf.nn.l2_loss(\n",
    "observed_keypoints_stop - predicted_keypoints)\n",
    "# Normalize by batch size and sequence length:\n",
    "vrnn_coord_pred_loss /= tf.to_float(\n",
    "  tf.shape(input_keypoint)[0] * tf.shape(input_keypoint)[1])\n",
    "train_model.add_loss(vrnn_coord_pred_loss)\n",
    "kl_loss = tf.reduce_mean(kl_divergence)  # Mean over batch and timesteps.\n",
    "train_model.add_loss(cfg.kl_loss_scale * kl_loss)\n",
    "vrnn_optimizer = tf.keras.optimizers.Adam(lr=cfg.learning_rate, clipnorm = cfg.clipnorm)\n",
    "train_model.compile(vrnn_optimizer)\n",
    "# Specify the checkpoint path\n",
    "checkpoint_path = \"VRNN_3883videos_vox_12-12.ckpt\" # 15-15 frames\n",
    "\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback to save the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                             save_weights_only=True,\n",
    "                                             verbose=1)\n",
    "\n",
    "print(\"Training VRNN_LEE prediction...\")\n",
    "train_model.fit(x=data_lee, steps_per_epoch = train_data.shape[0], epochs=10, callbacks=[cp_callback]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9050fe1a-6b18-42f6-b9c5-c797ab8fe3ee",
   "metadata": {},
   "source": [
    "# After VRNN is trained, load the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631fc218-f893-4e25-a74a-15333bfa0666",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_vrnn\n",
    "frames = 24\n",
    "cfg = get_config()\n",
    "input_keypoint = tf.keras.Input(shape=[frames,10,6],name='keypoint')\n",
    "observed_keypoints_stop = tf.keras.layers.Lambda(tf.stop_gradient)(\n",
    "input_keypoint)\n",
    "vrnn_model = model(cfg)\n",
    "predicted_keypoints, kl_divergence = vrnn_model(observed_keypoints_stop)\n",
    "train_model = tf.keras.Model(inputs=[input_keypoint],outputs=[predicted_keypoints])\n",
    "vrnn_coord_pred_loss = tf.nn.l2_loss(\n",
    "observed_keypoints_stop - predicted_keypoints)\n",
    "# Normalize by batch size and sequence length:\n",
    "vrnn_coord_pred_loss /= tf.to_float(\n",
    "  tf.shape(input_keypoint)[0] * tf.shape(input_keypoint)[1])\n",
    "train_model.add_loss(vrnn_coord_pred_loss)\n",
    "kl_loss = tf.reduce_mean(kl_divergence)  # Mean over batch and timesteps.\n",
    "train_model.add_loss(cfg.kl_loss_scale * kl_loss)\n",
    "\n",
    "# Load saved model:\n",
    "cfg = get_config()\n",
    "checkpoint_path = \"VRNN_3883videos_vox_12-12.ckpt\" \n",
    "# checkpoint_path = \"VRNN_3883videos_vox_6-6.ckpt\" \n",
    "\n",
    "# Loads the weights\n",
    "train_model.load_weights(checkpoint_path)\n",
    "train_model.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be408d6",
   "metadata": {},
   "source": [
    "# Import keypoints of 44 VoxCeleb test videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cbe1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"kp_test_44_vox.pkl\", \"rb\") as f:\n",
    "    kp_time_series = pickle.load(f)\n",
    "len(kp_time_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdc9b27",
   "metadata": {},
   "source": [
    "# Convert list of keypoints to dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95da1f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "for video_idx in range(len(kp_time_series)):\n",
    "    kp_time_series[video_idx] = kp_time_series[video_idx]['kp']\n",
    "\n",
    "kp_dict_init = []\n",
    "for video_idx in range(len(kp_time_series)): # \n",
    "    init_mean = []\n",
    "    init_jacobian = []\n",
    "    for frame_idx in range(len(kp_time_series[video_idx])):\n",
    "        kp_mean = kp_time_series[video_idx][frame_idx]['value'].reshape(1,10,2)\n",
    "        kp_mean = torch.tensor(kp_mean)\n",
    "        kp_jacobian = kp_time_series[video_idx][frame_idx]['jacobian'].reshape(1,10,2,2)\n",
    "        kp_jacobian = torch.tensor(kp_jacobian)\n",
    "\n",
    "        init_mean.append(kp_mean)\n",
    "        init_jacobian.append(kp_jacobian)\n",
    "\n",
    "    init_mean = torch.cat(init_mean)\n",
    "    init_jacobian = torch.cat(init_jacobian)\n",
    "\n",
    "    init_mean = torch.reshape(init_mean,(1,init_mean.shape[0],init_mean.shape[1],init_mean.shape[2]))\n",
    "    init_jacobian = torch.reshape(init_jacobian,(1,init_jacobian.shape[0],10,2,2))\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        # add tensor to cuda\n",
    "        init_mean = init_mean.to('cuda:0')\n",
    "        init_jacobian = init_jacobian.to('cuda:0')\n",
    "\n",
    "    kp_dict_both = {\"value\":init_mean,\"jacobian\":init_jacobian}\n",
    "    kp_dict_init.append(kp_dict_both)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90273494",
   "metadata": {},
   "source": [
    "# Apply min-max std to keypoints and convert to batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075a2afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "kp_list_test = []\n",
    "for video_idx in range(len(kp_dict_init)):\n",
    "    kp_one_video = torch.cat((kp_dict_init[video_idx]['value'], kp_dict_init[video_idx]['jacobian'].reshape(1,-1,10,4)),dim=-1).reshape(-1,60)\n",
    "    kp_one_video_array = np.array(kp_one_video.cpu())\n",
    "    kp_list_test.append(kp_one_video_array)\n",
    "    \n",
    "#####  min-max std to 60 dimensions of selected one video ######\n",
    "kp_list_test_std = []\n",
    "min_list = []\n",
    "range_list = []\n",
    "for video_idx in range(len(kp_list_test)):\n",
    "    min_values = np.min(kp_list_test[video_idx],axis=0) # 60 mins of one selected video in the loop\n",
    "    max_values = np.max(kp_list_test[video_idx],axis=0) # 60 maxs of one selected video in the loop\n",
    "    range_values = max_values - min_values \n",
    "    kp_one_video_std = (kp_list_test[video_idx] - min_values) / range_values\n",
    "    kp_list_test_std.append(kp_one_video_std)\n",
    "    min_list.append(min_values)\n",
    "    range_list.append(range_values)\n",
    "\n",
    "trajs = kp_list_test_std\n",
    "print(len(trajs))\n",
    "print(trajs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9de646",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### convert into batches:\n",
    "frames = 24 # 24 frames or 12 frames\n",
    "input_frames = int(frames / 2)\n",
    "data_batch_test = []\n",
    "for t,x in enumerate(kp_list_test_std):\n",
    "    if x.shape[0] >= frames:\n",
    "        num_full_batches = x.shape[0] // frames\n",
    "        for arr in np.array_split(x[:num_full_batches * frames], num_full_batches):\n",
    "            data_batch_test.append(arr)\n",
    "print(f'test dataset batches:', len(data_batch_test))\n",
    "print(data_batch_test[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8872f207",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### test dataset:\n",
    "\n",
    "test_data_reshape = np.array(data_batch_test).reshape(-1,frames,60)\n",
    "test_data_reshape.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376c19e5-ee36-44cc-a8c7-7caff90e7d29",
   "metadata": {},
   "source": [
    "# Predict keypoints using trained model and check std keypoints MSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c9c688-c031-4a26-8a24-2720d2b21e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test dataset and process model.predict():\n",
    "\n",
    "validation_data = test_data_reshape\n",
    "\n",
    "validation_data_tensor = tf.convert_to_tensor(validation_data.reshape(-1,frames,10,6))\n",
    "pred = train_model.predict(validation_data_tensor)\n",
    "print(pred.shape)\n",
    "\n",
    "# MSE loss:\n",
    "mse = np.mean((np.array(validation_data_tensor[:,input_frames:,:,:])- np.array(pred[:,input_frames:,:,:]))**2)\n",
    "rounded_mse = round(mse, 4)\n",
    "print(f'MSE: ', rounded_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19b86c4-b761-48d9-ad9a-88908129633b",
   "metadata": {},
   "source": [
    "# Check unstd keypoints MSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d67c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save num_batches for each video:\n",
    "num_batch_video = []\n",
    "num_full_batches_all = 0\n",
    "for t,x in enumerate(kp_list_test_std):\n",
    "    if x.shape[0] >= frames:\n",
    "        num_full_batches = x.shape[0] // frames\n",
    "        num_full_batches_all += num_full_batches\n",
    "        num_batch_video.append(num_full_batches)\n",
    "print(f'number of batches of each video:', len(num_batch_video))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacf886e-0203-4086-bedb-4468e1cf4fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first half of frames: groundtruth; last half of frames: predicted\n",
    "test_gt_pred = np.concatenate((test_data_reshape[:,:input_frames], pred.reshape(-1,frames,60)[:,input_frames:]), axis = 1)\n",
    "test_gt_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc63c71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unstd for each video:\n",
    "test_video_unstd_list = []\n",
    "for video_idx in range(len(num_batch_video)):\n",
    "    test_video = test_gt_pred[sum(num_batch_video[:video_idx]):sum(num_batch_video[:video_idx+1])]\n",
    "    test_video_unstd = test_video * range_list[video_idx] + min_list[video_idx]\n",
    "    test_video_unstd_list.append(test_video_unstd) # unstd video keypoints\n",
    "# unstd for each video of groundtruth data for MSE loss with std:\n",
    "validation_gt_unstd_list = []\n",
    "for video_idx in range(len(num_batch_video)):\n",
    "    validation_gt = validation_data[sum(num_batch_video[:video_idx]):sum(num_batch_video[:video_idx+1])]\n",
    "    validation_gt_unstd = validation_gt * range_list[video_idx] + min_list[video_idx]\n",
    "    validation_gt_unstd_list.append(validation_gt_unstd) # unstd video keypoints "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ea5cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE loss of unstd test data:\n",
    "validation_gt_unstd_array = np.concatenate(validation_gt_unstd_list, axis=0)\n",
    "print(validation_gt_unstd_array[:,input_frames:].shape) # last 12 gt frames \n",
    "\n",
    "test_video_unstd_array = np.concatenate(test_video_unstd_list, axis=0)\n",
    "print(test_video_unstd_array[:,input_frames:].shape) # last 12 pred frames\n",
    "\n",
    "mse = np.mean((test_video_unstd_array[:,input_frames:]- validation_gt_unstd_array[:,input_frames:])**2)\n",
    "rounded_mse = round(mse, 4)\n",
    "print(f'MSE: ', rounded_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b74a397-20c6-4395-b7b9-c14de9621770",
   "metadata": {},
   "source": [
    "# Optical flow and generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c8cf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### call the config functions and inference dataloader #########\n",
    "config=\"/home/jovyan/srinjoy-gan-vol/keypoints_project/config/abs-vox.yml\"\n",
    "\n",
    "# Test dataset\n",
    "with open(config) as f:\n",
    "    config = yaml.safe_load(f)\n",
    "dataset = FramesDataset(is_train=(False), **config['dataset_params'],mode=\"RNN\") # test\n",
    "\n",
    "print(len(dataset))\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=1)\n",
    "\n",
    "### call the functions        \n",
    "generator = OcclusionAwareGenerator(**config['model_params']['generator_params'],\n",
    "                                        **config['model_params']['common_params'])\n",
    "kp_detector = KPDetector(**config['model_params']['kp_detector_params'],\n",
    "                            **config['model_params']['common_params'])\n",
    "\n",
    "log_dir=\"/home/jovyan/srinjoy-gan-vol/keypoints_project/log/test-reconstruction-vox\"\n",
    "checkpoint=\"/home/jovyan/srinjoy-gan-vol/keypoints_project/Training_Prediction/FOMM/Trained_Models/vox-cpk.pth.tar\"\n",
    "\n",
    "if checkpoint is not None:\n",
    "    Logger.load_cpk(checkpoint, generator=generator, kp_detector=kp_detector)\n",
    "else:\n",
    "    raise AttributeError(\"Checkpoint should be specified for mode='reconstruction'.\")\n",
    "    \n",
    "def save_obj(obj, name ):\n",
    "    with open('./'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('./' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "png_dir = os.path.join(log_dir, 'prediction/png')\n",
    "log_dir = os.path.join(log_dir, 'prediction')\n",
    "\n",
    "if checkpoint is not None:\n",
    "    Logger.load_cpk(checkpoint, generator=generator, kp_detector=kp_detector)\n",
    "else:\n",
    "    raise AttributeError(\"Checkpoint should be specified for mode='reconstruction'.\")\n",
    "    \n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "if not os.path.exists(png_dir):\n",
    "    os.makedirs(png_dir)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    generator = DataParallelWithCallback(generator)\n",
    "    kp_detector = DataParallelWithCallback(kp_detector)\n",
    "\n",
    "generator = DataParallelWithCallback(generator)\n",
    "kp_detector = DataParallelWithCallback(kp_detector)\n",
    "\n",
    "generator.eval()\n",
    "kp_detector.eval()\n",
    "\n",
    "prediction_params = config['prediction_params']\n",
    "\n",
    "num_epochs = prediction_params['num_epochs']\n",
    "lr = prediction_params['lr']\n",
    "bs = prediction_params['batch_size']\n",
    "num_frames = prediction_params['num_frames']\n",
    "loss_list_total = []\n",
    "fvd_list_total = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09470afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########  FOMM+VRNN ########\n",
    "\n",
    "for it, x in tqdm(enumerate(dataloader)):\n",
    "        if config['reconstruction_params']['num_videos'] is not None:\n",
    "            if it > config['reconstruction_params']['num_videos']:\n",
    "                break\n",
    "        with torch.no_grad():\n",
    "            predictions = []\n",
    "            visualizations = []\n",
    "\n",
    "            ######## keypoints ########\n",
    "            kp_driving_video = test_video_unstd_list[it].reshape(-1,10,6)\n",
    "            kp_driving_video = torch.tensor(kp_driving_video)\n",
    "            kp_source = {\"value\":kp_driving_video[0,:,:2].reshape(1,10,2),\"jacobian\":kp_driving_video[0,:,2:].reshape(1,10,2,2)} # kp of the ith frame      \n",
    "        \n",
    "        ##### Start generator\n",
    "        loss_list = []\n",
    "        fvd_list = []\n",
    "        for i in range(((x['video'].shape[2])//frames)*frames): # cut the last <24 frames\n",
    "            source = x['video'][:, :, 0]\n",
    "            driving = x['video'][:, :, i]\n",
    "            kp_driving = {\"value\":kp_driving_video[i,:,:2],\"jacobian\":kp_driving_video[i,:,2:]} # kp of the ith frame\n",
    "            kp_driving['value'] = kp_driving['value'].reshape(1,10,2)\n",
    "            kp_driving['jacobian'] = kp_driving['jacobian'].reshape(1,10,2,2)\n",
    "            out = generator(source, kp_source=kp_source, kp_driving=kp_driving)\n",
    "            out['kp_source'] = kp_source\n",
    "            out['kp_driving'] = kp_driving\n",
    "            del out['sparse_deformed']\n",
    "            predictions.append(np.transpose(out['prediction'].data.cpu().numpy(), [0, 2, 3, 1])[0])\n",
    "\n",
    "            visualization = Visualizer(**config['visualizer_params']).visualize(source=source,\n",
    "                                                                                    driving=driving, out=out)\n",
    "            visualizations.append(visualization)\n",
    "            # mse loss\n",
    "            if np.abs(out['prediction'].detach().cpu().numpy() - driving.cpu().numpy()).mean() != 0:\n",
    "                loss_list.append(np.abs(out['prediction'].detach().cpu().numpy() - driving.cpu().numpy()).mean())\n",
    "                # Calculate FVD for each frame using ground truth and predicted videos\n",
    "                ground_truth_features = driving.detach().cpu().permute(0,2,3,1).reshape(256,256,3)\n",
    "                predicted_features = out['prediction'].detach().cpu().permute(0,2,3,1).reshape(256,256,3)\n",
    "                fvd_list.append(compute_fvd(ground_truth_features, predicted_features))\n",
    "\n",
    "        print(\"Reconstruction loss: %s\" % np.mean(loss_list))\n",
    "        loss_list_total.append(np.mean(loss_list))\n",
    "\n",
    "        print(\"FVD Score: %s\" % np.mean(fvd_list))\n",
    "        fvd_list_total.append(np.mean(fvd_list))\n",
    "\n",
    "        predictions = np.concatenate(predictions, axis=1)\n",
    "        imageio.imsave(os.path.join(png_dir, x['name'][0] + '.png'), (255 * predictions).astype(np.uint8))\n",
    "        image_name = x['name'][0] + config['reconstruction_params']['format']\n",
    "        imageio.mimsave(os.path.join(log_dir, image_name), visualizations)\n",
    "\n",
    "print(\"mean Reconstruction loss: %s\" % np.mean(loss_list_total)) \n",
    "print(\"mean FVD score: %s\" % np.mean(fvd_list_total)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
